<div class="page-header">
	<h1 style="">{{appName}}</h1>
</div>
<div style="float: right"><img src="images/Streaming.svg"></div>
<p>The amount of datasets in the world of Linked Data is growing each day. Yet, extracting and counting links in a efficient manner was one of the challenges which the community still struggles to solve. <!-- The creation of diagrams which
	visualize the Data Web with a rich level of meta-data is a hard task and a crucial challenge of any analysis is to provide
	accurate methods to analyze links between data. A second challenge is the freshness of such an analysis. Especially in the
	case of large amount of big datasets, which are hosted in a decentralized way, detecting and counting links requires a centralized
	index and methods that are scalable enough to keep up-to-date. --> {{appName}} closes this gap providing a service that extract and count links among RDF dump files in the streaming process. </p>
	
	
	<div class="page-header" style="width: 600px">
	<h3>Do you have a RDF dump file published on the web?</h3>
</div>
	
	<p> Click in the <a href="#/quick">"Quick Start"</a> menu and check how easy is to use our API to verify links (indegree and outdegree) of your dump file.</p>
<p>A detailed description of this work can be found in
	<a href="http://svn.aksw.org/papers/2015/ISWC_DynLOD/public.pdf">this paper</a>. The basic documentation is available in our <a href="wiki.html">Wiki</a> page.</p>
<div class="page-header">
	<h3>A brief explanation...</h3>
</div>
<p>{{appName}} uses Bloom filters (BF) to compare and <b>count links between RDF dump files in the streamming process.</b> How do we do that?
	<br>
	<ul>
		<li>1. {{appName}} parses your dataset description file that might be in different formats such as VoID, DCAT and DataID. Then, we stream your distributions
			(dump files) in order to extract links and compare with different Bloom filters wich contains index for other dump files.</li>
		<li>2. We retrieve data about links (<b>void:linkset</b> format) of your dataset and we create new Bloom filters representing your dump file. Later these Bloom filters will be used to compare with other distributions.</li>
		<!--<li>3. A <b>LOD-Diagram</b> is provided showing the links that we discovered of your datasets.</li>-->
		<li>3. You can use a REST API to retrieve links indegree and outdegree of your dump files.</li>
	</ul>
</p>
<p>We provide an online demo which uses our API. Click in <a href="#/quick">"Quick Start"</a> and check how easy is to use {{appName}}.</p> 

<!--you can use to add your dataset, retrieve links and a customized diagram.</p>-->
<!--<div class="page-header">
	<h4>Create your own diagram based on the linksets.</h4>
</div>Considering that you can describe your datasets with different properties, our API creates customized SVG diagrams. The
following example diagram shows all distribution of English DBpedia, vocabularies and three NIF datasets.
<div style="border: 1px solid #d8d8d8; background-color: #ffffff; margin-top: 40px; text-align: center">
	<img alt="" src="resources/img/cloud.png">
</div>-->
<div class="page-header">
	<h3>How fast and reliable is {{appName}}?</h3>
</div>
<div>The following table shows 5 experiments. We added the 48 English <a href="http://downloads.dbpedia.org/3.9/en/">DBpedia distributions (dump files)</a> and counted links among them. Then, we added all 358 vocabularies available in
	<a href="http://lov.okfn.org/dataset/lov/">LOV</a> (referred as round 2), and again we counted links among the vocabularies and DBpedia distributions. In the last three experiments, we added three NIF converted corpora: Reuters128, RSS-500 and Brown Corpus (which contains 123 distributions). We measured false positive rate and time to count links.
	
	<table class="table" style="margin-top:35px">
		<th>Round</th>
		<th>Dataset</th>
		<th>Dist.</th>
		<th>Triples</th>
		<th>
			<i>tp</i>
		</th>
		<th>
			<i>fp</i>
		</th>
		<th>Precision</th>
		<th>F-Measure</th>
		<th>Time</th>
		<tr>
			<td>1</td>
			<td>English DBpedia</td>
			<td>48</td>
			<td>812M</td>
			<td>9.013.302.727</td>
			<td>1.354</td>
			<td>0,99999985</td>
			<td>0,999999925</td>
			<td>01:43:20</td>
		</tr>
		<tr>
			<td>2</td>
			<td>LOV Vocabularies</td>
			<td>358</td>
			<td>862K</td>
			<td>31.027.759</td>
			<td>77</td>
			<td>0,99999752</td>
			<td>0.999998759</td>
			<td>00:01:12</td>
		</tr>
		<tr>
			<td>3</td>
			<td>Routers128</td>
			<td>1</td>
			<td>7k</td>
			<td>2501</td>
			<td>0</td>
			<td>1</td>
			<td>1</td>
			<td>00:00:21</td>
		</tr>
		<tr>
			<td>4</td>
			<td>RSS-500</td>
			<td>1</td>
			<td>10k</td>
			<td>1265</td>
			<td>0</td>
			<td>1</td>
			<td>1</td>
			<td>00:00:33</td>
		</tr>
		<tr>
			<td>5</td>
			<td>Brown Corpus</td>
			<td>123</td>
			<td>3.4M</td>
			<td>890.760</td>
			<td>3</td>
			<td>0.99999663</td>
			<td>0.999998316</td>
			<td>00:04:04</td>
		</tr>
	</table>We are not the counting time to stream each dump file, only the time to compare and count links.</div>
<!--div class="page-header">
	<h3>How can I see a customized cloud for my dataset?</h3>
</div>
<div>After adding you dataset, to retrieve a customized cloud diagram you can access:
	<br>http://vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/cloud.html?dataset=http://myDomain.org/myDataset
	<p>
		<a href="http://vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/cloud.html?dataset=http://downloads.dbpedia.org/2014/commons/article_categories_commons.nt.bz2">This
								example</a>shows a diagram for a DBpedia distribution.</p>Want more examples? Check our
	<a href="wiki.html#diagram">Wiki
							page</a>and find out with details how to add datasets to the cloud.</div -->
<div class="page-header">
	<h3>How can I add dataset to {{appName}}?</h3>
</div>
<div>It's simple. You can add a dataset using our API, and you need to specify what's the file format.
	<br>For instance, to add a file "http://myDomain.org/void.ttl" in turtle format, you must access:
	<br>
	<code>http://vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/api?addDataset=http://myDomain.org/void.ttl&rdfFormat=ttl</code>
	<br>The
	<code>addDataset</code> parameter accepts VoID, DCAT and DataID descriptions. {{appName}} request the header "Accept: application/rdf+xml"
	in order to retrive data in RDF format. The
	<code>rdfFormat</code> parameter accept <b>"nt"</b>, <b>"ttl"</b>, or <b>"xmlrdf"</b>and is the format of the <code>addDataset</code> parameter.
	<p>Our <a href="wiki.html">Wiki</a> has the necessary documentation that you need to add datasets in to the cloud.</p>
</div>
<div class="page-header">
	<h3>How to check id my dataset was successfully added?</h3>
</div>
<div>To check the current status of your dataset, you can access:
	<br>http://vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/api?datasetStatus=http://myDomain.org/myDataset
	<p>
		<a href="http://vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/api?datasetStatus=https://raw.githubusercontent.com/cirola2000/DynamicLOD/master/src/main/webapp/dataids_example/dataidDbpediaEnglish.ttl">This example</a> shows the current status for DBpedia dataset.</p>
</div>
<div class="page-header">
	<h3>How to retrieve
		<b>void:linkset</b> data to my dataset?</h3>
</div>
<div>To retrieve linksets of your dataset, you can access:
	<br>http://vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/api?retrieveDataset=http://myDomain.org/myDataset
	<p>
		<a href="http://vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/api?retrieveDataset=http://dbpedia.org/dataid.ttl#DBpedia_en_3.9_article_categories_en">This example</a> shows linksets for a DBpedia distribution.</p>
</div>
<div class="page-header">
	<h3>How to get {{appName}}?</h3>
</div>{{appName}} is an open-source project and it's maintained by the
<a href="http://aksw.org/About.html">AKSW</a> group from
<a href="http://www.zv.uni-leipzig.de/">University of Leipzig</a>. You can download and deploy the project from our
<a href="https://github.com/AKSW/dynamiclod">source code</a>. {{appName}} uses
<a href="http://d3js.org/"> Data Driven Documents JS Library</a> to create a cloud-based diagram.