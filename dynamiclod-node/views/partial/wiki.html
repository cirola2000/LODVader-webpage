<div class="page-header">
	<h1 style="">{{appName}} Wiki</h1>
</div>
<p>This page provides a basic guideline for {{appName}} API usage. Further information about the software architecture can be
	found in <a href="http://svn.aksw.org/papers/2015/ISWC_DynLOD/public.pdf">this paper</a>.</p>
<div class="page-header">
	<h2>Contents</h2>
</div>
<div>
	<ul>
		<!--<li>
				<a href="#onlineDemo">Using our Online demo</a>
			</li>-->
		<!--<li>
				<a href="#diagram">Diagram Overview</a>
			</li>-->
		<li>
			<a href="/#/wiki/#api">API usage</a>
		</li>
		<ul>
			<li>
				<a href="/#/wiki/#example">Example of a description file.</a>
			</li>
			<li>
				<a href="/#/wiki/#adding">Adding dataset to diagram</a>
			</li>
			<li>
				<a href="/#/wiki/#checking">Checking your dataset status</a>
			</li>
			<li>
				<a href="/#/wiki/#retrieving">Retrieving RDF VoID Linkset data</a>
			</li>
		</ul>
		<li>
			<a href="/#/wiki/#downloading">Downloading and installing your stand-alone {{appName}}</a>
		</li>
	</ul>
</div>
<!--<div class="page-header">
		<h2>
			<a name="onlineDemo"></a>Using our Online demo</h2>
	</div>
	<div>We provide a free Online instance of {{appName}}. The API is available at the address
		<a href="{{serverURL}}api">{{serverURL}}api</a>, and for the diagram visualization, we provide the address
		<a href="{{serverURL}}cloud.html?dataset=">{{serverURL}}cloud.html?dataset=</a>.
		<br>
	</div>
	<div class="page-header">
		<h2>
			<a name="diagram"></a>Diagram Overview</h2>
	</div>
	<div>
		<p>{{appName}} provides a
			<a href="{{serverURL}}cloud.html?dataset=">diagram</a>which you can add your datasets, and find out whether your dataset contain linkset to other datasets. Linksets are defined
			as intersections of objects and subjects, which means that links occurs when objects and subjects of distinct datasets are
			equals. The visualization consists of two parts, which includes a tree of datasets and an area which is updates with a SVG
			diagram according to the selected datasets of the tree.</p>
		<p>The root of the tree contains the top datasets, i.e. datasets which don't contains a parent dataset. The following figure
			depicts the structure of DBpedia commons datasets:</p>
		<div style="text-align: center; margin-top: 25px; margin-bottom: 25px">
			<img style="border: 1px solid #d8d8d8;" alt="" src="resources/img/tree2.png">
		</div>Notice that the top dataset named "DBpedia 2014 commons dumps dataset" contains several subsets (categories, links, degrees,
		etc), and each subset contains a dump file (distribution). You can select one or more item to filter and create a customized
		graph. The following figure depicts the generated diagram for the selected distributions.
		<div style="text-align: center; margin-top: 25px; margin-bottom: 25px">
			<img style="border: 1px solid #d8d8d8;" alt="" src="resources/img/diagram_tree.png">
		</div>That there are three different nodes (bubbles) colors. The grey nodes are the nodes selected from the tree, green nodes
		the related datasets, and finally the orange nodes are ontologies. The arrows represents the links direction between the
		nodes. Notice that if your dataset has not been shown in the graph, means the current dataset doesn't have more that 50 links.
		<p>Try now some of the following examples and check out the resulted diagram! In the top level of granularity, check out where
			lod-cloud.net is connected with:</p>
		<pre>
			<a href="{{serverURL}}cloud.html?dataset=http://lod-cloud.net/">{{serverURL}}cloud.html?dataset=http://lod-cloud.net/</a>
		</pre>
		<!-- p>
						You can also choose a distribution level, in this case "DBpedia article categories" distribution:
						</p>
						<pre><a href="{{serverURL}}cloud.html?dataset=http://downloads.dbpedia.org/2014/commons/article_categories_commons.nt.bz2">{{serverURL}}cloud.html?dataset=http://downloads.dbpedia.org/2014/commons/article_categories_commons.nt.bz2</a></pre ->
		<p>With no parameters, you can see a general overview of all datasets in a multiple granularity level:</p>
		<pre>
			<a href="{{serverURL}}cloud.html">{{serverURL}}cloud.html?dataset=</a>
		</pre>
	</div>-->
<div class="page-header">
	<h2>
		<a name="api"></a>API usage</h2>
</div>
<div><p>{{appName}} API has four main features, which consists:</p>
	<ul>
		<li>Add new dataset to be streamed.</li>
		<li>Retrieve status about the current streaming.</li>
		<li>Retrieve RDF using <samp>void:linkset</samp> format.</li>
		<li>Retrieve JSON data for diagram generation.</li>
	</ul>
	
	<p>For all four features, the API returns a JSON object. The following sections describe the API usage.</p></div>
	
<h2 style="margin-top: 30px">
	<a name="example"></a>Example of a description file.</h2>
<div>{{appName}} parses different description files, according to the vocabularies
	<a href="http://wiki.dbpedia.org/projects/dbpedia-dataid-unit"
	target="_blank">DataID</a>,
	<a href="http://www.w3.org/TR/void/" target="_blank">VoID</a> and
	<a href="http://www.w3.org/TR/vocab-dcat/" target="_blank">DCAT</a>. Thus, basically what you need to add your datasets is a link that contains the description file of your dataset in one of the above formats. {{appName}} will read it and try to fetch dump files described by the properties <samp>dcat:downloadURL</samp>, <samp>dcat:accessURL</samp> and <samp>void:dataDump</samp>. 
	<p>The following basic RDF code describes using the <a href="http://www.w3.org/TR/vocab-dcat/" target="_blank">DCAT</a> vocabulary the News-100 NIF dataset: </p>
	<pre>

@prefix dcat: &lt;http://www.w3.org/ns/dcat#> .
@prefix void: &lt;http://rdfs.org/ns/void#> .
@prefix sd: &lt;http://www.w3.org/ns/sparql-service-description#> .
@prefix prov: &lt;http://www.w3.org/ns/prov#> .
@prefix dc: &lt;http://purl.org/dc/terms/> .
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#> .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/> .

&lt;http://gerbil.aksw.org/gerbil/dataId/corpora/N3-News-100#dataset>
  a dcat:Dataset ;
  dc:title "News-100 NIF NER Corpus" ;
  rdfs:label "News-100 NIF NER Corpus" ;
  dcat:distribution &lt;https://raw.githubusercontent.com/AKSW/n3-collection/master/News-100.nt> .


&lt;https://raw.githubusercontent.com/AKSW/n3-collection/master/News-100.nt>
  a dcat:Distribution ;
  dc:title "Complete corpus" ;
  dc:description "Complete corpus file in turtle format" ;
  dcat:downloadURL "https://raw.githubusercontent.com/AKSW/n3-collection/master/News-100.nt" ;
  dc:format "text/ntriples" .
 </pre>
 
 <p>The parser will fetch all properties from <samp>dcat:Dataset</samp> and then will follow <samp>dcat:distribution</samp> resource. In this case, the property <samp>dcat:downloadURL</samp> is available and the parser will try to stream the content located at the <samp>https://raw.githubusercontent.com/AKSW/n3-collection/master/News-100.nt </samp> address.</p> 

<h2 style="margin-top: 30px">
	<a name="adding"></a>Adding new dataset to be streamed.</h2>
	 Considering that the application requests the HTTP header in the format <samp>"Accept: application/rdf+xml"</samp> it is also possible, for instance, to add your dataset via CKAN link.
	
	<p>The API parameters used to add a dataset are:</p>
	<table class="table" style="margin-top: 20px; margin-bottom: 20px">
		<th>Parameter:</th>
		<th>Description:</th>
		<th>Accepts array?</th>
		<th style="width: 300px">Returns:</th>
		<tr>
			<td>
				<samp>addDataset</samp>
			</td>
			<td>link for your dataset description. Might be a list of links.</td>
			<td>
				<b>Yes</b>
			</td>
			<td rowspan="2">
				<b>JSON Object</b> with a core message "API initialized" and a parser message with the number of dump files founded.</td>
		</tr>
		<tr>
			<td>
				<samp>rdfFormat</samp>
			</td>
			<td>format of the added links in the
				<samp>addDataset</samp> parameter. Formats are:
				<b>ttl</b>,
				<b>nt</b> or
				<b>rdfxml</b>.</td>
			<td>
				<b>No</b>
			</td>
			<td></td>
		</tr>
	</table>
	<p>An generic example of a link to add a http://x.y.x/dataset.ttl file in turtle is:
		<samp>vmdbpedia.informatik.uni-leipzig.de:9090/dynlod/api?addDataset=http://x.y.x/dataset.ttl?rdfFormat=ttl</samp>.</p>
	<p>After adding the dataset, you should get a JSON Object indicating whether the API was successfully initiated:</p>
		<pre>
{
  "parserMsg": {"message": "1 distributions found. We are processing them!"},
  "coreMsg": "API successfully initialized."
}
		</pre>
		
		
		<p> To check details about
		parsed distributions and datasets, check the next section for the
		<i>datasetStatus</i>parameter.</p>
</div>
<h2 style="margin-top: 30px">
	<a name="checking"></a>Checking your dataset status</h2>
<div>
	<p>The API parameter used to verify the details of the loading/streaming process for a dataset is:</p>
	<table class="table">
		<th>Parameter</th>
		<th>Description</th>
		<th>Accepts array?</th>
		<th>Returns</th>
		<tr>
			<td>
				<samp>datasetStatus</samp>
			</td>
			<td>link for your previously added dataset.</td>
			<td>
				<b>No</b>
			</td>
			<td>
				<b>JSON Object</b>
			</td>
		</tr>
	</table>
	
	<p>A detailed description for the returned <b>JSON Object</b> of the <samp>datasetStatus</samp> parameter is given by the next table.</p>
	<!--<table class="table">
		<th style="width: 150px">Key</th>
		<th>Description</th>
		<th style="width: 350px; padding-left: 10px">Possible values</th>
		<tr>
			<td>downloadURL</td>
			<td>Identify the distributions which being dexcribed.</td>
			<td>The distribution URL</td>
		</tr>
		<tr>
			<td>success</td>
			<td>
				<b>boolean</b>value. Case value is false, an error occured and you can check the "lastErrorMessage" or the "message" value.</td>
			<td>
				<b>true</b>or
				<b>false</b>
			</td>
		</tr>
		<tr>
			<td>lastErrorMessage</td>
			<td>A friendly message indicating the error case "success" field is
				<b>false</b>.</td>
			<td>-</td>
		</tr>
		<tr>
			<td>message</td>
			<td>General message returned from server.</td>
			<td>-</td>
		</tr>
		<tr>
			<td>topDataset</td>
			<td>Returns the top parent dataset of the described distribution.</td>
			<td>Top dataset URI</td>
		</tr>
		<tr>
			<td>status</td>
			<td>The current status for the distribution.</td>
			<td>Described in the next table</td>
		</tr>
	</table>-->
	<p>Even if you load thousands of distributions, the "status" key allows you to check individually the status of each of them.
	The next table details, in order, each possible value for the key "status". </p>
	<table class="table">
		<th>Value</th>
		<th>Description</th>
		<tr>
			<td>
				<samp>WAITING_TO_STREAM</samp>
			</td>
			<td>Your distribution is in the download queue and was not streamed yet.</td>
		</tr>
		<tr>
			<td>
				<samp>STREAMING</samp>
			</td>
			<td>Your distribution being streamed/downloaded.</td>
		</tr>
		<tr>
			<td>
				<samp>CREATING_BLOOM_FILTER</samp>
			</td>
			<td>Your distribution was successfully streamed and {{appName}} is creating the Bloom filter.</td>
		</tr>
		<tr>
			<td>
				<samp>DONE</samp>
			</td>
			<td>The process was complete with no errors.</td>
		</tr>
		<tr>
			<td>
				<samp>ERROR</samp>
			</td>
			<td>Something went wrong and the error is described in the "lastErrorMessage" field.</td>
		</tr>
	</table>The following example was produced for a N3-Reuters-128 dataset description file, and depicts the status for the <samp>https://raw.githubusercontent.com/AKSW/n3-collection/master/Reuters-128.nt</samp> ditribution:
	<pre style="margin-top: 20px">... 
		
{
    "coreMsg": "API successfully initialized.",
    "distributions": [{
        "lastTimeStreamed": "11:02:00 25/08/2015",
        "triples": 6967,
        "defaultDatasets": ["http://gerbil.aksw.org/gerbil/dataId/corpora/N3-Reuters-128#dataset"],
        "outdegree": [
            {
                "isVocabulary": true,
                "targetDistribution": "http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#",
                "sourceDataset": "http://gerbil.aksw.org/gerbil/dataId/corpora/N3-Reuters-128#dataset",
                "sourceDistribution": "https://raw.githubusercontent.com/AKSW/n3-collection/master/Reuters-128.nt",
                "links": 1265,
                "targetDataset": "http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#"
            },
            {
                "targetDistribution": "http://purl.org/twc/lodcloud/source/tw-rpi-edu/file/cr-full-dump/version/latest/conversion/purl-org-twc-lodcloud.nt.gz",
                "sourceDataset": "http://gerbil.aksw.org/gerbil/dataId/corpora/N3-Reuters-128#dataset",
                "sourceDistribution": "https://raw.githubusercontent.com/AKSW/n3-collection/master/Reuters-128.nt",
                "links": 98,
                "targetDataset": "http://wifo5-36.informatik.uni-mannheim.de:5000/dataset/the-living-lod-cloud"
            }
        ],
        "downloadUrl": "https://raw.githubusercontent.com/AKSW/n3-collection/master/Reuters-128.nt",
        "lastMsg": "Distribution is already in the last version. No needs to stream again.",
        "resourceUri": "http://gerbil.aksw.org/gerbil/dataId/corpora/N3-Reuters-128#dataset",
        "title": "Complete corpus",
        "indegree": [
            {
                "targetDistribution": "https://raw.githubusercontent.com/AKSW/n3-collection/master/Reuters-128.nt",
                "sourceDataset": "http://gerbil.aksw.org/gerbil/dataId/corpora/N3-News-100#dataset",
                "sourceDistribution": "https://raw.githubusercontent.com/AKSW/n3-collection/master/News-100.nt",
                "links": 1956,
                "targetDataset": "http://gerbil.aksw.org/gerbil/dataId/corpora/N3-Reuters-128#dataset"
            },
            {
                "targetDistribution": "https://raw.githubusercontent.com/AKSW/n3-collection/master/Reuters-128.nt",
                "sourceDataset": "http://gerbil.aksw.org/gerbil/dataId/corpora/N3-RSS-500#dataset",
                "sourceDistribution": "https://raw.githubusercontent.com/AKSW/n3-collection/master/RSS-500.nt",
                "links": 2501,
                "targetDataset": "http://gerbil.aksw.org/gerbil/dataId/corpora/N3-Reuters-128#dataset"
            }
        ],
        "status": "DONE"
    }]
}
		
		</pre>
		<p>If all status are equal to <samp>DONE</samp> or <samp>ERROR</samp>, means that {{appName}} already processed all of them, and there is nothing else to do. Case there are errors
			which you can fix, you might be able to send again your description file via
			<samp>addDataset</samp> parameter. {{appName}} will not stream again distributions already in the
			<samp>DONE</samp> status, unless its detected a change in the file size. {{appName}} do it by requesting the HTTP "Content-Length" header
			and comparing the previous distribution size.</p>
</div>
<h2 style="margin-top: 30px">
	<a name="retrieving"></a>Retrieving RDF VoID Linkset data</h2>
<div>The following table describes the <samp>retrieveDataset</samp>, which retrieves RDF data about counted links in the
	<b>VoID Linkset</b>format.
	<table class="table">
		<th>Parameter</th>
		<th>Description</th>
		<th>Accepts array?</th>
		<th>Returns</th>
		<tr>
			<td>
				<samp>retrieveDataset</samp>
			</td>
			<td>the dataset/distribution URI</td>
			<td>
				<b>No</b>
			</td>
			<td>RDF (turtle format) with linksets data.</td>
		</tr>
	</table>
</div>
<p>The following example shows links from ontos-news-portal to DBpedia_en_3.9_labels_en distribution: </p>
<pre>
	&lt;{{serverURL}}?retrieveDataset&source=http://wifo5-36.informatik.uni-mannheim.de:5000/dataset/ontos-news-portal&target=http://dbpedia.org/dataid.ttl#DBpedia_en_3.9_labels_en>
        a                    void:Linkset ;
        void:objectsTarget   &lt;http://wifo5-36.informatik.uni-mannheim.de:5000/dataset/ontos-news-portal> ;
        void:subjectsTarget  &lt;http://dbpedia.org/dataid.ttl#DBpedia_en_3.9_labels_en> ;
        void:triples         "475" ;
        prov:wasGeneratedBy  &lt;{{serverURL}}> .
</pre>


<div class="page-header">
	<h4 style="margin-top: 30px">
		<a name="downloading"></a>Downloading and installing your stand-alone {{appName}}</h4>
</div>
<div>To deploy {{appName}} in your own server, please check our
	<a href="https://github.com/AKSW/dynamiclod">GitHub page</a>.</div>